# SE-project
SummarEase is a software application designed to provide students with a centralized summary of study materials under the format of long or short documents, book chapters, articles, with an emphasis on scientific literature. 
In order to fine tune the llama2-7b model, we used a trimmed version of the arxiv-summarization dataset from huggingface. (https://huggingface.co/datasets/ccdv/arxiv-summarization) in order to match our GPU capabilities. The dataset 
contains a large volume of snippets from scientific articles, journals and book chapters that allow us to customize the language model to give abstract-like output which is particularly helpful for students in STEM.

The app uses two key AI components: a fine-tuned Llama 2 model and a translation model. The Llama 2 model, which is a powerful language model fine-tuned on scientific literature, forms the backbone of the document summarization feature. It 
can understand and generate human-like text, providing accurate, concise summaries of complex academic papers. The translation model allows the app to support multiple languages, ensuring that students around the world can benefit from the
service. It likely translates the summaries generated by the Llama 2 model into the userâ€™s preferred language.

We trained the 7b version for 2 epochs and saved final weights and biases obtained by training using Parameter efficient fine tuning(PEFT), Low rank adaptation(LORA) method. On top of it, we used the bart translation model for different
languages.

For scaling the system, we have decided to migrate the application to AWS instances. To launch an instance, we have installed an apache server on Ubuntu, and further configured mysql and php stack to deploy the html. Security group was
adjusted accordingly. Also, the flask server was configured to run the actual model. You can try our application by accessing: http://54.226.137.162/ 
